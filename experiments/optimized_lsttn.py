# experiments/optimized_lsttn_regression.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import os
import time
import math
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å·¥å…·å‡½æ•°
from utils import load_data, evaluate_model, plot_predictions, save_results_to_csv
torch.backends.cudnn.enabled = False

class MultiScaleSpectralAttention(nn.Module):
    """å¤šå°ºåº¦å…‰è°±æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.scales = [3, 7, 15, 31]  # ä¸åŒå°ºåº¦çš„å·ç§¯æ ¸
        self.scale_convs = nn.ModuleList([
            nn.Conv1d(input_dim, hidden_dim // len(self.scales), 
                     kernel_size=scale, padding=scale//2)
            for scale in self.scales
        ])
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8, batch_first=True)
        self.norm = nn.LayerNorm(hidden_dim)
        
        # ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–
        self.attention_weights = None
        
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        x_conv = x.transpose(1, 2)  # (batch, input_dim, seq_len)
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        scale_features = []
        for conv in self.scale_convs:
            feat = conv(x_conv)  # (batch, hidden_dim//4, seq_len)
            scale_features.append(feat)
        
        # æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾
        multi_scale = torch.cat(scale_features, dim=1)  # (batch, hidden_dim, seq_len)
        multi_scale = multi_scale.transpose(1, 2)  # (batch, seq_len, hidden_dim)
        
        # è‡ªæ³¨æ„åŠ›
        attended, attention_weights = self.attention(multi_scale, multi_scale, multi_scale)
        self.attention_weights = attention_weights  # ä¿å­˜æ³¨æ„åŠ›æƒé‡
        output = self.norm(attended + multi_scale)
        
        return output

class SpectralPositionalEncoding(nn.Module):
    """å…‰è°±ä¸“ç”¨ä½ç½®ç¼–ç """
    def __init__(self, d_model, max_len=200):  # æ”¹ä¸º200
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # æ·»åŠ å…‰è°±é¢‘ç‡ç›¸å…³çš„ç¼–ç 
        spectral_factor = torch.linspace(0, 1, max_len).unsqueeze(1)
        pe = pe * (1 + 0.1 * spectral_factor)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:seq_len].unsqueeze(0)

class AdaptivePatchEmbedding(nn.Module):
    """è‡ªé€‚åº”patchåµŒå…¥"""
    def __init__(self, input_dim, embed_dim, patch_sizes=[8, 12, 16]):
        super().__init__()
        self.patch_sizes = patch_sizes
        self.embed_dim = embed_dim
        
        # ç¡®ä¿èƒ½æ­£ç¡®åˆ†é…ç»´åº¦
        if embed_dim % len(patch_sizes) != 0:
            # å¦‚æœä¸èƒ½æ•´é™¤ï¼Œæ‰‹åŠ¨åˆ†é…
            base_dim = embed_dim // len(patch_sizes)
            remainder = embed_dim % len(patch_sizes)
            dims = [base_dim + (1 if i < remainder else 0) for i in range(len(patch_sizes))]
        else:
            dims = [embed_dim // len(patch_sizes)] * len(patch_sizes)
        
        # å¤šä¸ªä¸åŒå¤§å°çš„patchåµŒå…¥
        self.patch_embeds = nn.ModuleList([
            nn.Conv1d(1, dim, kernel_size=ps, stride=ps)
            for ps, dim in zip(patch_sizes, dims)
        ])
        
        # è‡ªé€‚åº”æƒé‡
        self.patch_weights = nn.Parameter(torch.ones(len(patch_sizes)))
        
    def forward(self, x):
        # x: (batch, 1, 700)
        patch_features = []
        
        for i, (patch_embed, patch_size) in enumerate(zip(self.patch_embeds, self.patch_sizes)):
            # ç¡®ä¿èƒ½è¢«patch_sizeæ•´é™¤
            seq_len = x.size(2)
            trimmed_len = (seq_len // patch_size) * patch_size
            x_trimmed = x[:, :, :trimmed_len]
            
            feat = patch_embed(x_trimmed)  # (batch, dim, num_patches)
            patch_features.append(feat)
        
        # åŠ æƒèåˆä¸åŒpatch sizeçš„ç‰¹å¾
        weights = F.softmax(self.patch_weights, dim=0)
        weighted_features = []
        min_patches = min([feat.size(2) for feat in patch_features])
        
        for i, feat in enumerate(patch_features):
            feat_trimmed = feat[:, :, :min_patches]
            weighted_features.append(feat_trimmed * weights[i])
        
        # æ‹¼æ¥ç‰¹å¾
        output = torch.cat(weighted_features, dim=1)  # (batch, embed_dim, num_patches)
        return output.transpose(1, 2)  # (batch, num_patches, embed_dim)

class EnhancedTransformerBlock(nn.Module):
    """å¢å¼ºçš„Transformerå—"""
    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):
        super().__init__()
        
        # æ ‡å‡†æ³¨æ„åŠ›
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
        # å·ç§¯å‰é¦ˆç½‘ç»œï¼ˆæ›´é€‚åˆå…‰è°±æ•°æ®ï¼‰
        self.conv_ffn = nn.Sequential(
            nn.Conv1d(d_model, dim_feedforward, 1),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Conv1d(dim_feedforward, d_model, 1),
            nn.Dropout(dropout)
        )
        
        # ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–
        self.attention_weights = None
        
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ›
        attn_out, attention_weights = self.self_attn(x, x, x)
        self.attention_weights = attention_weights  # ä¿å­˜æ³¨æ„åŠ›æƒé‡
        x = self.norm1(x + self.dropout(attn_out))
        
        # å·ç§¯å‰é¦ˆ
        x_conv = x.transpose(1, 2)  # (batch, d_model, seq_len)
        ffn_out = self.conv_ffn(x_conv)
        ffn_out = ffn_out.transpose(1, 2)  # (batch, seq_len, d_model)
        
        x = self.norm2(x + ffn_out)
        return x

class OptimizedSpectralLSTTN(nn.Module):
    """ä¼˜åŒ–ç‰ˆ LSTTN å…‰è°±å›å½’æ¨¡å‹"""
    def __init__(self, input_dim=700, output_dim=4, hidden_dim=128):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        
        # 1. è‡ªé€‚åº”patchåµŒå…¥
        self.patch_embedding = AdaptivePatchEmbedding(
            input_dim, hidden_dim, patch_sizes=[8, 12, 16]
        )
        
        # 2. å…‰è°±ä¸“ç”¨ä½ç½®ç¼–ç 
        self.pos_encoding = SpectralPositionalEncoding(hidden_dim, max_len=512) 
        
        # 3. å¤šå°ºåº¦å…‰è°±æ³¨æ„åŠ›
        self.multi_scale_attn = MultiScaleSpectralAttention(hidden_dim, hidden_dim)
        
        # 4. å¢å¼ºTransformerç¼–ç å™¨
        self.transformer_layers = nn.ModuleList([
            EnhancedTransformerBlock(hidden_dim, nhead=8, dim_feedforward=hidden_dim*4)
            for _ in range(6)  # å¢åŠ å±‚æ•°
        ])
        
        # 5. å¤šè·¯å¾„ç‰¹å¾æå–å™¨
        # å…¨å±€è·¯å¾„ï¼šæ•´ä½“è¶‹åŠ¿
        self.global_extractor = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # å±€éƒ¨è·¯å¾„ï¼šå…³é”®åŒºåŸŸ
        self.local_extractor = nn.Sequential(
            nn.Conv1d(hidden_dim, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveMaxPool1d(1),
            nn.Flatten(),
            nn.Dropout(0.2)
        )
        
        # åºåˆ—è·¯å¾„ï¼šæ—¶åºä¿¡æ¯
        self.sequence_extractor = nn.LSTM(
            hidden_dim, 32, batch_first=True, bidirectional=True
        )
        
        # 6. è‡ªæ³¨æ„åŠ›èåˆ
        self.fusion_attention = nn.MultiheadAttention(
            embed_dim=64+64+64, num_heads=8, batch_first=True
        )
        
        # 7. ä»»åŠ¡ç‰¹å®šé¢„æµ‹å¤´
        task_dims = {'moisture': 32, 'starch': 32, 'oil': 48, 'protein': 48}  # æ ¹æ®éš¾åº¦è°ƒæ•´
        
        self.task_heads = nn.ModuleDict({
            task: nn.Sequential(
                nn.Linear(64+64+64, dim),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(dim, dim//2),
                nn.ReLU(),
                nn.Linear(dim//2, 1)
            ) for task, dim in task_dims.items()
        })
        
        # 8. æ®‹å·®è¿æ¥çš„æœ€ç»ˆé¢„æµ‹
        self.final_predictor = nn.Sequential(
            nn.Linear(64+64+64, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, output_dim)
        )
        
    def forward(self, x):
        batch_size = x.shape[0]
        
        # 1. æ·»åŠ é€šé“ç»´åº¦å¹¶è¿›è¡ŒpatchåµŒå…¥
        x = x.unsqueeze(1)  # (batch, 1, 700)
        x = self.patch_embedding(x)  # (batch, num_patches, hidden_dim)
        
        # 2. ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        
        # 3. å¤šå°ºåº¦æ³¨æ„åŠ›
        x = self.multi_scale_attn(x)
        
        # 4. Transformerç¼–ç  (ä¿å­˜æ¯å±‚çš„æ³¨æ„åŠ›æƒé‡)
        attention_weights_list = []
        for layer in self.transformer_layers:
            x = layer(x)
            if layer.attention_weights is not None:
                attention_weights_list.append(layer.attention_weights)
        
        # ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–
        self.attention_weights_list = attention_weights_list
        
        # 5. å¤šè·¯å¾„ç‰¹å¾æå–
        x_transpose = x.transpose(1, 2)  # (batch, hidden_dim, seq_len)
        
        # å…¨å±€ç‰¹å¾
        global_feat = self.global_extractor(x_transpose)  # (batch, 64)
        
        # å±€éƒ¨ç‰¹å¾
        local_feat = self.local_extractor(x_transpose)  # (batch, 64)
        
        # åºåˆ—ç‰¹å¾
        seq_feat, _ = self.sequence_extractor(x)  # (batch, seq_len, 64)
        seq_feat = seq_feat.mean(dim=1)  # (batch, 64)
        
        # 6. ç‰¹å¾èåˆ
        combined_feat = torch.cat([global_feat, local_feat, seq_feat], dim=1)  # (batch, 192)
        
        # è‡ªæ³¨æ„åŠ›èåˆ
        fused_feat = combined_feat.unsqueeze(1)  # (batch, 1, 192)
        fused_feat, attention_weights_fusion = self.fusion_attention(fused_feat, fused_feat, fused_feat)
        self.fusion_attention_weights = attention_weights_fusion  # ä¿å­˜èåˆæ³¨æ„åŠ›æƒé‡
        fused_feat = fused_feat.squeeze(1)  # (batch, 192)
        
        # 7. ä»»åŠ¡ç‰¹å®šé¢„æµ‹
        task_outputs = []
        task_names = ['moisture', 'starch', 'oil', 'protein']
        
        for task in task_names:
            task_out = self.task_heads[task](fused_feat)
            task_outputs.append(task_out)
        
        task_pred = torch.cat(task_outputs, dim=1)  # (batch, 4)
        
        # 8. æœ€ç»ˆé¢„æµ‹ï¼ˆæ®‹å·®è¿æ¥ï¼‰
        final_pred = self.final_predictor(fused_feat)
        
        # åŠ æƒèåˆä»»åŠ¡ç‰¹å®šå’Œé€šç”¨é¢„æµ‹
        alpha = 0.7  # å¯å­¦ä¹ å‚æ•°
        output = alpha * task_pred + (1 - alpha) * final_pred
        
        return output

def visualize_attention_weights(model, X_sample, sample_idx=0, save_dir="/ssd1/zhanghongbo04/002/project/NIR-Corn/experiments/experiments/results/attention_maps", device='cpu'):
    """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡"""
    os.makedirs(save_dir, exist_ok=True)
    torch.backends.cudnn.enabled = False

    model.eval()
    model.to(device)
    
    with torch.no_grad():
        X_tensor = torch.FloatTensor(X_sample).unsqueeze(0).to(device)
        _ = model(X_tensor)
        
        # å¯è§†åŒ–å¤šå°ºåº¦æ³¨æ„åŠ›
        if hasattr(model.multi_scale_attn, 'attention_weights') and model.multi_scale_attn.attention_weights is not None:
            attn_weights = model.multi_scale_attn.attention_weights
            print(f"âœ… å¤šå°ºåº¦æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
            print(f"æ³¨æ„åŠ›æƒé‡ç±»å‹: {type(attn_weights)}")
            
            # æ£€æŸ¥ä¸åŒçš„ç»´åº¦æƒ…å†µ
            if isinstance(attn_weights, torch.Tensor):
                if len(attn_weights.shape) == 4:  # (batch, heads, seq_len, seq_len)
                    attn_map = attn_weights[0, 0, :, :].cpu().numpy()
                elif len(attn_weights.shape) == 3:  # (heads, seq_len, seq_len)
                    attn_map = attn_weights[0, :, :].cpu().numpy()
                elif len(attn_weights.shape) == 2:  # (seq_len, seq_len)
                    attn_map = attn_weights.cpu().numpy()
                else:
                    print(f"æœªçŸ¥çš„æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
                    return
                
                plt.figure(figsize=(10, 8))
                sns.heatmap(attn_map, cmap='viridis', cbar=True)
                plt.title(f'Multi-Scale Attention Weights - Sample {sample_idx}')
                plt.xlabel('Key Positions')
                plt.ylabel('Query Positions')
                plt.tight_layout()
                save_path = os.path.join(save_dir, f'multi_scale_attention_sample_{sample_idx}.png')
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                plt.close()
                print(f"âœ… ä¿å­˜å¤šå°ºåº¦æ³¨æ„åŠ›å›¾: {save_path}")
            else:
                print("æ³¨æ„åŠ›æƒé‡ä¸æ˜¯Tensorç±»å‹")
        
        # å¯è§†åŒ–Transformerå„å±‚æ³¨æ„åŠ›
        if hasattr(model, 'attention_weights_list') and model.attention_weights_list:
            for layer_idx, attn_weights in enumerate(model.attention_weights_list):
                print(f"Transformerå±‚ {layer_idx+1} æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
                if isinstance(attn_weights, torch.Tensor):
                    if len(attn_weights.shape) == 4:  # (batch, heads, seq_len, seq_len)
                        attn_map = attn_weights[0, 0, :, :].cpu().numpy()
                    elif len(attn_weights.shape) == 3:  # (heads, seq_len, seq_len)
                        attn_map = attn_weights[0, :, :].cpu().numpy()
                    elif len(attn_weights.shape) == 2:  # (seq_len, seq_len)
                        attn_map = attn_weights.cpu().numpy()
                    else:
                        print(f"æœªçŸ¥çš„æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
                        continue
                    
                    plt.figure(figsize=(10, 8))
                    sns.heatmap(attn_map, cmap='viridis', cbar=True)
                    plt.title(f'Transformer Layer {layer_idx+1} Attention Weights - Sample {sample_idx}')
                    plt.xlabel('Key Positions')
                    plt.ylabel('Query Positions')
                    plt.tight_layout()
                    save_path = os.path.join(save_dir, f'transformer_layer_{layer_idx+1}_attention_sample_{sample_idx}.png')
                    plt.savefig(save_path, dpi=300, bbox_inches='tight')
                    plt.close()
                    print(f"âœ… ä¿å­˜Transformerå±‚{layer_idx+1}æ³¨æ„åŠ›å›¾: {save_path}")
                else:
                    print("æ³¨æ„åŠ›æƒé‡ä¸æ˜¯Tensorç±»å‹")

def train_optimized_lsttn_model(model, train_loader, val_loader, epochs=150, lr=0.0008, device='cpu'):
    """è®­ç»ƒä¼˜åŒ–ç‰ˆLSTTNæ¨¡å‹"""
    
    # å¤šä»»åŠ¡æŸå¤±å‡½æ•°
    def multi_task_loss(pred, target):
        # åŸºç¡€MSEæŸå¤±
        mse_loss = F.mse_loss(pred, target)
        
        # L1æ­£åˆ™åŒ–
        l1_loss = F.l1_loss(pred, target)
        
        # ä»»åŠ¡å¹³è¡¡æŸå¤±
        task_losses = []
        for i in range(target.size(1)):
            task_loss = F.mse_loss(pred[:, i], target[:, i])
            task_losses.append(task_loss)
        
        # åŠ¨æ€æƒé‡è°ƒæ•´
        task_weights = torch.tensor([1.2, 1.0, 1.5, 2.0]).to(device)  # æ ¹æ®ä»»åŠ¡éš¾åº¦è°ƒæ•´
        weighted_loss = sum(w * l for w, l in zip(task_weights, task_losses))
        
        return 0.7 * mse_loss + 0.2 * l1_loss + 0.1 * weighted_loss
    
    # ä¼˜åŒ–å™¨è®¾ç½® - å¢åŠ å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ - è°ƒæ•´å‚æ•°
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=lr, epochs=epochs, 
        steps_per_epoch=len(train_loader),
        pct_start=0.2, anneal_strategy='cos'
    )
    
    model.to(device)
    best_loss = float('inf')
    patience = 30  # å¢åŠ è€å¿ƒ
    patience_counter = 0
    
    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = multi_task_loss(outputs, y_batch)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = multi_task_loss(outputs, y_batch)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        if epoch % 20 == 0:
            print(f"Epoch {epoch}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {avg_val_loss:.6f}")
        
        # æ—©åœæœºåˆ¶
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), "experiments/models/optimized_lsttn_best.pth")
        else:
            patience_counter += 1
            if patience_counter >= patience and epoch > 80:  # å¢åŠ æœ€å°è®­ç»ƒè½®æ•°
                print(f"Early stopping at epoch {epoch}")
                break
    
    return model

def run_optimized_lsttn_experiment():
    """è¿è¡Œä¼˜åŒ–ç‰ˆLSTTNå›å½’å®éªŒ"""
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆ LSTTN å›å½’å®éªŒ...")
    
    # åŠ è½½æ•°æ®
    (X_train, X_test, y_train_scaled, y_test_scaled, 
     scaler_X, scaler_y, y_train_orig, y_test_orig) = load_data()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # def augment_spectral_data(X, y, noise_level=0.005, shift_range=2):  # é™ä½å¢å¼ºå¼ºåº¦
    #     """è½»é‡çº§å…‰è°±æ•°æ®å¢å¼º"""
    #     augmented_X, augmented_y = [], []
        
    #     for i in range(len(X)):
    #         # åŸå§‹æ•°æ®
    #         augmented_X.append(X[i])
    #         augmented_y.append(y[i])
            
    #         # æ·»åŠ è½»å¾®å™ªå£°
    #         noise = np.random.normal(0, noise_level, X[i].shape)
    #         augmented_X.append(X[i] + noise)
    #         augmented_y.append(y[i])
            
    #         # è½»å¾®å…‰è°±åç§»
    #         shift = np.random.randint(-shift_range, shift_range+1)
    #         if shift != 0:
    #             shifted_x = np.roll(X[i], shift)
    #             augmented_X.append(shifted_x)
    #             augmented_y.append(y[i])
        
    #     return np.array(augmented_X), np.array(augmented_y)
    
    # åº”ç”¨è½»é‡çº§æ•°æ®å¢å¼º
    # X_train_aug, y_train_scaled_aug = augment_spectral_data(X_train, y_train_scaled)

    X_train_aug, y_train_scaled_aug = X_train, y_train_scaled
    print(f"ğŸ“ˆ è½»é‡çº§æ•°æ®å¢å¼ºå: {X_train_aug.shape[0]} æ ·æœ¬")
    
    # è½¬æ¢ä¸ºå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train_aug)
    y_train_tensor = torch.FloatTensor(y_train_scaled_aug)
    X_test_tensor = torch.FloatTensor(X_test)
    y_test_tensor = torch.FloatTensor(y_test_scaled)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader, TensorDataset
    
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)  # å‡å°batch size
    
    val_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # åˆ›å»ºä¼˜åŒ–æ¨¡å‹ - è°ƒæ•´è¶…å‚æ•°
    model = OptimizedSpectralLSTTN(input_dim=700, output_dim=4, hidden_dim=128)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ§  æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    # è®­ç»ƒæ¨¡å‹ - è°ƒæ•´è®­ç»ƒå‚æ•°
    print("ğŸ§  è®­ç»ƒä¼˜åŒ–ç‰ˆ LSTTN æ¨¡å‹...")
    model = train_optimized_lsttn_model(
        model, train_loader, val_loader, 
        epochs=250, lr=0.001, device=device  # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œæé«˜å­¦ä¹ ç‡
    )
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load("experiments/models/optimized_lsttn_best.pth", map_location=device))
    
    # é¢„æµ‹
    print("ğŸ”® è¿›è¡Œé¢„æµ‹...")
    model.eval()
    with torch.no_grad():
        X_test_tensor = X_test_tensor.to(device)
        y_pred_scaled = model(X_test_tensor).cpu().numpy()
    
    y_pred_orig = scaler_y.inverse_transform(y_pred_scaled)
    
    # è¯„ä¼°
    metrics = evaluate_model(
        y_test_orig, y_pred_orig, 
        model_name="Optimized-LSTTN",
        scaler_y=None
    )
    
    # ç”»å›¾
    plot_predictions(
        y_test_orig, y_pred_orig,
        model_name="Optimized LSTTN",
        save_path="experiments/results/optimized_lsttn_predictions.png"
    )
    
    # æ³¨æ„åŠ›å¯è§†åŒ–
    print("ğŸ¨ ç”Ÿæˆæ³¨æ„åŠ›çƒ­åŠ›å›¾...")
    # é€‰æ‹©å‡ ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    sample_indices = [0, 1, 2, 5, 10]  # é€‰æ‹©å‰å‡ ä¸ªæ ·æœ¬
    for idx in sample_indices:
        if idx < len(X_test):
            visualize_attention_weights(model, X_test[idx], idx, device=device)  # ä¼ é€’è®¾å¤‡å‚æ•°
    
    print("ğŸ“Š æ³¨æ„åŠ›çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: experiments/results/attention_maps/")
    
    # ä¿å­˜ç»“æœ
    save_results_to_csv(metrics, "Optimized-LSTTN")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = "experiments/models"
    os.makedirs(model_dir, exist_ok=True)
    
    torch.save(model.state_dict(), os.path.join(model_dir, "optimized_lsttn_model.pth"))
    joblib.dump(scaler_X, os.path.join(model_dir, "optimized_lsttn_scaler_X.pkl"))
    joblib.dump(scaler_y, os.path.join(model_dir, "optimized_lsttn_scaler_y.pkl"))
    
    print("âœ… ä¼˜åŒ–ç‰ˆ LSTTN å›å½’å®éªŒå®Œæˆ")
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
    
    return metrics

if __name__ == "__main__":
    run_optimized_lsttn_experiment()