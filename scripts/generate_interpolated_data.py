# scripts/generate_interpolated_data.py

import numpy as np
import pandas as pd
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import joblib
import matplotlib.pyplot as plt

def load_and_preprocess_data(csv_path):
    """åŠ è½½å¹¶é¢„å¤„ç†åŸå§‹æ•°æ®"""
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {csv_path}")
    df = pd.read_csv(csv_path)
    
    # æå–å…‰è°±å’Œæ ‡ç­¾
    spectrum_cols = [col for col in df.columns if col.startswith("Wave_")]
    X = df[spectrum_cols].values  # (80, 700)
    
    target_names = ['Moisture', 'Starch', 'Oil', 'Protein']
    y = df[target_names].values   # (80, 4)
    
    print(f"âœ… åŸå§‹æ•°æ®å½¢çŠ¶: å…‰è°± {X.shape}, æ ‡ç­¾ {y.shape}")
    return X, y, spectrum_cols, target_names

def generate_interpolated_samples(X_real, y_real, num_samples=920, noise_level=0.0):
    """
    ç”Ÿæˆæ’å€¼æ ·æœ¬
    
    Parameters:
    - X_real: åŸå§‹å…‰è°± (80, 700)
    - y_real: åŸå§‹æ ‡ç­¾ (80, 4)
    - num_samples: è¦ç”Ÿæˆçš„æ ·æœ¬æ•°
    - noise_level: å¯é€‰å™ªå£°æ°´å¹³ (0.0 = æ— å™ªå£°)
    """
    print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {num_samples} ä¸ªæ’å€¼æ ·æœ¬...")
    
    n_real = X_real.shape[0]
    X_interpolated = []
    y_interpolated = []
    
    for i in range(num_samples):
        # éšæœºé€‰æ‹©ä¸¤ä¸ªæ ·æœ¬
        idx1, idx2 = np.random.choice(n_real, 2, replace=False)
        
        # éšæœºæ’å€¼ç³»æ•°
        alpha = np.random.uniform(0.1, 0.9)  # é¿å…ç«¯ç‚¹
        
        # æ’å€¼è®¡ç®—
        interpolated_spectrum = alpha * X_real[idx1] + (1 - alpha) * X_real[idx2]
        interpolated_label = alpha * y_real[idx1] + (1 - alpha) * y_real[idx2]
        
        # å¯é€‰ï¼šæ·»åŠ å°å™ªå£°
        if noise_level > 0:
            noise_spec = np.random.normal(0, noise_level, size=interpolated_spectrum.shape)
            interpolated_spectrum += noise_spec
        
        X_interpolated.append(interpolated_spectrum)
        y_interpolated.append(interpolated_label)
    
    X_interpolated = np.array(X_interpolated)
    y_interpolated = np.array(y_interpolated)
    
    print(f"âœ… ç”Ÿæˆå®Œæˆ: å…‰è°± {X_interpolated.shape}, æ ‡ç­¾ {y_interpolated.shape}")
    return X_interpolated, y_interpolated

def save_processed_data(X_real, y_real, X_fake, y_fake, output_dir="data/processed_interpolated"):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®ï¼ˆåŒ…æ‹¬å½’ä¸€åŒ–ï¼‰"""
    print(f"ğŸ’¾ ä¿å­˜å¤„ç†åæ•°æ®åˆ°: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # å½’ä¸€åŒ–
    X_scaler = StandardScaler()
    y_scaler = MinMaxScaler(feature_range=(0, 1))
    
    # å¯¹çœŸå®+æ’å€¼æ•°æ®ä¸€èµ·å½’ä¸€åŒ–ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
    X_combined = np.vstack([X_real, X_fake])
    y_combined = np.vstack([y_real, y_fake])
    
    X_scaled = X_scaler.fit_transform(X_combined)
    y_scaled = y_scaler.fit_transform(y_combined)
    
    # åˆ†ç¦»å›çœŸå®å’Œæ’å€¼æ•°æ®
    n_real = X_real.shape[0]
    X_real_scaled = X_scaled[:n_real]
    X_fake_scaled = X_scaled[n_real:]
    y_real_scaled = y_scaled[:n_real]
    y_fake_scaled = y_scaled[n_real:]
    
    # ä¿å­˜å½’ä¸€åŒ–åçš„æ•°æ®
    np.save(os.path.join(output_dir, "X_real_scaled.npy"), X_real_scaled)
    np.save(os.path.join(output_dir, "y_real_scaled.npy"), y_real_scaled)
    np.save(os.path.join(output_dir, "X_interpolated_scaled.npy"), X_fake_scaled)
    np.save(os.path.join(output_dir, "y_interpolated_scaled.npy"), y_fake_scaled)
    
    # ä¿å­˜åŸå§‹å€¼ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    np.save(os.path.join(output_dir, "X_real_original.npy"), X_real)
    np.save(os.path.join(output_dir, "y_real_original.npy"), y_real)
    np.save(os.path.join(output_dir, "X_interpolated_original.npy"), X_fake)
    np.save(os.path.join(output_dir, "y_interpolated_original.npy"), y_fake)
    
    # ä¿å­˜ scaler
    joblib.dump(X_scaler, os.path.join(output_dir, "X_scaler.pkl"))
    joblib.dump(y_scaler, os.path.join(output_dir, "y_scaler.pkl"))
    
    print("âœ… æ•°æ®ä¿å­˜å®Œæˆ")
    return X_scaler, y_scaler

def visualize_samples(X_real, X_fake, y_real, y_fake, num_samples=5):
    """å¯è§†åŒ–çœŸå® vs æ’å€¼æ ·æœ¬"""
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾...")
    
    # ç¡®ä¿ä¸è¶…è¿‡å®é™…æ ·æœ¬æ•°
    num_samples = min(num_samples, len(X_real), len(X_fake))
    
    # å…‰è°±å¯¹æ¯” (æœ€å¤šæ˜¾ç¤º 5 ä¸ª)
    plt.figure(figsize=(15, 10))
    
    # ç¬¬ä¸€è¡Œï¼šå…‰è°±å¯¹æ¯”
    for i in range(min(4, num_samples)):  # æœ€å¤š 4 ä¸ªå…‰è°±å¯¹æ¯”
        plt.subplot(2, 4, i+1)
        plt.plot(X_real[i], alpha=0.8, label='Real', linewidth=1)
        plt.plot(X_fake[i], '--', alpha=0.8, label='Interpolated', linewidth=1)
        plt.title(f'Spectrum {i+1}')
        plt.ylabel('Intensity')
        if i == 0:
            plt.legend(fontsize=8)
        plt.grid(True, alpha=0.3)
    
    # ç¬¬äºŒè¡Œï¼šæ ‡ç­¾åˆ†å¸ƒå¯¹æ¯”
    target_names = ['Moisture', 'Starch', 'Oil', 'Protein']
    for i in range(4):
        plt.subplot(2, 4, 4 + i + 1)  # ä»ç¬¬ 5 ä¸ªä½ç½®å¼€å§‹
        plt.hist(y_real[:, i], bins=15, alpha=0.7, label='Real', color='blue', density=True)
        plt.hist(y_fake[:, i], bins=30, alpha=0.7, label='Interpolated', color='orange', density=True)
        plt.xlabel(target_names[i], fontsize=8)
        plt.ylabel('Density', fontsize=8)
        plt.title(f'{target_names[i]}', fontsize=10)
        if i == 3:  # åªåœ¨æœ€åä¸€ä¸ªå›¾ä¾‹æ˜¾ç¤º
            plt.legend(fontsize=8)
        plt.grid(True, alpha=0.3)
        plt.tick_params(axis='both', labelsize=6)
    
    plt.tight_layout()
    output_path = "src/saved_samples/interpolation_comparison.png"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
def main():
    # è·¯å¾„è®¾ç½®
    csv_path = "/ssd1/zhanghongbo04/002/project/NIR-Corn/data/raw/corn_mp5_regression_data.csv"
    output_dir = "data/processed_interpolated"
    
    # 1. åŠ è½½åŸå§‹æ•°æ®
    X_real, y_real, spectrum_cols, target_names = load_and_preprocess_data(csv_path)
    
    # 2. ç”Ÿæˆæ’å€¼æ ·æœ¬
    X_fake, y_fake = generate_interpolated_samples(
        X_real, y_real, 
        num_samples=1920, 
        noise_level=0.01  # æ·»åŠ è½»å¾®å™ªå£°ä½¿æ ·æœ¬æ›´è‡ªç„¶
    )
    
    # 3. ä¿å­˜æ•°æ®
    X_scaler, y_scaler = save_processed_data(X_real, y_real, X_fake, y_fake, output_dir)
    
    # 4. å¯è§†åŒ–å¯¹æ¯”
    visualize_samples(X_real, X_fake, y_real, y_fake)
    
    # 5. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"åŸå§‹æ ·æœ¬æ•°: {X_real.shape[0]}")
    print(f"æ’å€¼æ ·æœ¬æ•°: {X_fake.shape[0]}")
    print(f"æ€»æ ·æœ¬æ•°: {X_real.shape[0] + X_fake.shape[0]}")
    
    for i, name in enumerate(target_names):
        real_min, real_max = y_real[:, i].min(), y_real[:, i].max()
        fake_min, fake_max = y_fake[:, i].min(), y_fake[:, i].max()
        print(f"{name}: çœŸå®[{real_min:.2f}, {real_max:.2f}] ç”Ÿæˆ[{fake_min:.2f}, {fake_max:.2f}]")

if __name__ == "__main__":
    main()